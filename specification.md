---
title: Platform Technical Specification
description: Detailed engineering specifications for the dFDA core platform implementation.
published: false
date: 2025-07-26T14:00:00.000Z
tags: [features, platform, specification, engineering]
editor: markdown
dateCreated: 2025-07-26T14:00:00.000Z
---

# Platform Technical Specification

**Status:** Draft / Elaboration Complete (Illustrative)
**Version:** 0.3.0

> **Document Purpose:** This document provides detailed technical specifications for implementation. It is the engineering counterpart to the high-level conceptual overview found in the **[Platform Architecture document](./architecture.md)**.

This document provides detailed technical specifications for the implementation of the core platform, aligning with the architecture in [/features/platform/03-platform.md](/features/platform/03-platform.md) and enabling functionalities discussed in related documents like [/features/data-import.md](/features/data-import.md), [/features/data-silo-api-gateways.md](/features/data-silo-api-gateways.md), and drawing inspiration from efficiency models like [/reference/recovery-trial.md](/reference/recovery-trial.md). It is intended for engineers, developers, and technical teams.

**Note on Implementation Strategy:** The primary implementation strategy is to **fund and integrate existing, best-in-class open-source solutions** through a public prize and grant program, rather than building the entire platform from the ground up. The specific technologies listed in this document are therefore **illustrative candidates** that meet the architectural requirements. They serve as a reference blueprint for evaluating potential open-source projects for integration.

## 1. Introduction

* **Purpose:** To define the specific technologies, interfaces, data structures, security measures, and infrastructure requirements for building the global health protocol initiated by the United States.
* **Relationship:** Serves as the detailed implementation guide based on the architecture described in `architecture.md`.
* **Scope:** The Platform is designed to function as a global public utility and **Backend-as-a-Service (BaaS)**, serving as the **reference implementation** of the global health protocol. This specification focuses on the core backend components (API, Storage, Mapping/Validation, Access Control) and the interfaces for a plugin framework. It excludes the internal implementation details of specific plugins or third-party applications built upon the platform. All core software developed shall be open-source.

## 2. Technology Stack

Selected to prioritize scalability, security, interoperability, developer productivity, and leveraging mature ecosystems suitable for health data.

* **Open Source License:** All code developed for the platform shall be released under the **GNU General Public License v3.0**, as mandated by `SEC. 204(b)` of the governing Act.
* **Programming Languages:**
    * Backend/API: **Python 3.11+** (FastAPI framework, strong data science/validation libraries (Pandas, Pydantic), mature async support, large talent pool).
    * Frontend (Reference UI): **TypeScript** with **React 18+** (Robust typing, component model, large ecosystem, Next.js framework). Must support internationalization (i18n) for multi-language accessibility as mandated by `SEC. 204(l)`.
    * Data Processing/Mapping Engine: **Python 3.11+** (Leverages Pandas, potentially Dask/Spark for large-scale transformations).
* **Frameworks:**
    * Web API: **FastAPI** (Performance, auto-validation, OpenAPI generation).
    * Web Frontend: **Next.js** (SSR, routing, optimized React development).
    * Data Processing: Standard Python libraries + **Dask** (for parallelizing mapping/validation jobs where appropriate).
* **Database Technologies:**
    * Metadata/Relational Storage: **PostgreSQL 15+ (Managed Service - e.g., AWS RDS)** (ACID compliance, JSONB support, maturity, availability of extensions like PostGIS).
    * Time-Series Data Storage: **TimescaleDB (Managed Service - e.g., Timescale Cloud or via Managed PostgreSQL)** (Optimized time-series performance on mature PostgreSQL foundation, supports analytical queries).
    * Raw Data/File Storage: **AWS S3** (Scalability, durability, cost-effectiveness, lifecycle policies, event notifications).
* **Cloud Provider & Core Services:**
    * Primary Cloud Provider: **AWS** (Mature, comprehensive services, regulatory compliance support - BAA for HIPAA). While AWS is the initial target for the reference implementation, the architecture should strive for **cloud-agnosticism** where possible by relying on standardized, container-based deployments to mitigate vendor lock-in and facilitate adoption by international partners using other cloud providers.
    * Compute: **AWS EKS** (Kubernetes for container orchestration), **AWS Lambda** (for specific event-driven tasks/APIs), **AWS EC2** (potentially for specialized workloads).
    * Storage: **AWS S3**, **AWS RDS (PostgreSQL)**, Managed **TimescaleDB**, **AWS EBS**.
    * Networking: **AWS VPC** (Isolation), **API Gateway** (Management, security), **Route 53** (DNS), **CloudFront** (CDN for frontend/static assets).
    * Messaging Queues: **AWS SQS** (Decoupled asynchronous processing), potentially **AWS SNS** (Notifications).
* **Containerization & Orchestration:**
    * Containerization: **Docker**.
    * Orchestration: **Kubernetes (AWS EKS)** (Scalability, resilience, standardized deployments).
* **Blockchain/Distributed Ledger Technology (DLT):**
    * **Status:** **Core Component.** Required to fulfill mandates in the governing Act (`SEC. 204(c)(4)`, `SEC. 204(i)`).
    * **Use Cases:**
        * **Supply-Chain Integrity:** An immutable ledger, interoperable with DSCSA, for tracking investigational products from manufacturer to patient.
        * **Privacy-Preserving Audit:** Patient-level transactions (e.g., data access, consent) shall be represented as **zero-knowledge proofs** on the ledger, allowing verification without revealing protected health information.
        * **Consent Management:** Hashing consent forms and permissions to the ledger for an immutable, timestamped record of user intent.
    * **Implementation:** To be integrated via a prize program. The platform will integrate with a suitable public L1/L2 protocol. Candidate ecosystems include **Polygon (PoS/zkEVM)**, **Starknet**, or other EVM-compatible chains with robust ZKP support. To maximize trust, preference will be given to ZKP schemes with transparent, non-trusted setups (e.g., **STARKs**).
* **Monitoring & Logging Tools:**
    * Logging: **AWS CloudWatch Logs** integrated with **OpenSearch/Elasticsearch** for centralized querying/analysis.
    * Monitoring/Metrics: **AWS CloudWatch Metrics** & **Prometheus/Grafana** (deployed on EKS for finer-grained infra/app metrics).
    * APM: **AWS X-Ray** or **Datadog**.
    * Error Tracking: **Sentry**.
    * **Automated Reporting (`SEC. 204(j)`):** The monitoring stack must be designed to support the automated generation of the annual public transparency report. This requires persistent storage of key metrics and a reporting service to query and publish the data.

## 3. Core Component Implementation Details

### 3.1 Data Ingestion API

Designed for flexibility, security, and standardization.

* **API Protocol:** **RESTful API** using JSON payloads.
* **Specification:** **OpenAPI Specification (OAS) v3.x**. Strictly maintained, versioned, and published. Auto-generated documentation via FastAPI.
* **Authentication:** **OAuth 2.0 Bearer Tokens** (JWTs) issued by the central Auth Service (see 3.4). Required for all endpoints.
* **Core Endpoints (Illustrative):**
    * `POST /v1/ingest/observations`: Submits structured time-series data batches conforming to the internal standardized schema. Returns `job_id` for async processing.
    * `POST /v1/ingest/files`: Uploads raw files (CSV, PDF, VCF, images etc.) via multipart/form-data. Returns `file_id` and `job_id`.
    * `GET /v1/ingest/jobs/{job_id}`: Retrieves status and outcome (success, failure, validation issues) of an ingestion job.
    * `GET /v1/variables?category={cat}&system={sys}&code={code}`: Retrieves reference variable definitions, filterable.
    * `GET /v1/units?system={sys}`: Retrieves reference unit definitions (e.g., UCUM).
* **Developer Portal and Access Tokens:**
    * To foster a vibrant ecosystem, a comprehensive **Developer Portal** shall be provided. It will include OpenAPI-generated documentation, tutorials, SDKs, and a registration portal for third-party applications.
    * The platform's Access Control module shall support user-generated **Personal Access Tokens**. Users must be able to create, label (e.g., "My Analysis Script"), monitor usage of, and revoke these tokens through their data portal.
* **Data Validation Rules (Server-side):**
    * Schema Validation: Performed by FastAPI/Pydantic against OAS.
    * Semantic Validation (Async job): Checks against `variables` definition (range, allowed units). Flags (`is_valid=false`) or rejects based on severity/configuration. Checks for temporal consistency where possible.
    * Duplicate Check (Async job): Prevents data points with identical `(user_id, variable_id, timestamp, source_id)`. Logs duplicates.
* **SDK Design:**
    * Targets: **Python**, **JavaScript/TypeScript**. Others based on demand.
    * Functionality: Handles OAuth flows, request building, file upload streaming, job status polling, reference data lookup.
    * Distribution: Open source (PyPI, npm).
* **Registration/Discovery:** Plugin Registry stores metadata, including required OAuth scopes, input/output data schemas, and execution endpoints. Core uses this to validate and route requests.

### 3.2 Trial Creator Workspace

This section details the backend services required to power the Trial Creator Workspace mandated by the Act.

* **E-Protocol Builder (`SEC. 204(c)(1)`)**
    * **Status:** Design Required.
    * **Description:** A system to allow trial creators to design, validate, and register study protocols.
    * **Technical Considerations:** Requires dedicated data models for protocol structure (phases, arms, eligibility criteria), a rules engine for automated compliance validation against 21 CFR Parts 312/812 and ISO 14155, and versioning capabilities. Will expose endpoints for creating and managing protocols. To encourage pragmatic and efficient trials, it should also include features like pre-built templates based on the RECOVERY trial, libraries of core outcome sets (COS) for common diseases, and tools to estimate the data collection burden of a given protocol.

* **Liability-Insurance Exchange (`SEC. 204(c)(2)`)**
    * **Status:** Design Required.
    * **Description:** A module to integrate with third-party insurance underwriters to provide real-time quotes for trial liability.
    * **Technical Considerations:** Requires secure APIs for partner integration, data models for quotes and policies, and integration with the E-Protocol Builder.

* **Trial Cost, Discount, and Deposit Module (`SEC. 204(c)(3)`)**
    * **Status:** Design Required.
    * **Description:** A financial module to manage patient-specific trial costs, application of NIH-funded discounts, and the collection/refund of data provision deposits.
    * **Technical Considerations:** Requires a secure financial transaction ledger, integration with external payment gateways, and a rules engine for applying discounts. Must be highly auditable.

### 3.3 Trial Execution and Clinical Site Integration

This section outlines the components necessary to execute trials, particularly adaptive and pragmatic trials, and to integrate seamlessly with clinical workflows, as exemplified by the RECOVERY trial.

* **Adaptive Trial Engine**
    * **Status:** Design Required.
    * **Description:** A core service responsible for managing the operational logic of adaptive trials. This is critical for efficiency, allowing trials to change based on interim data.
    * **Technical Considerations:** Must support various designs (e.g., platform trials, basket trials, response-adaptive randomization). Will include a rules engine to implement protocol-defined adaptation logic (e.g., dropping a treatment arm for futility, adjusting randomization ratios). Requires a secure interface for Data and Safety Monitoring Boards (DSMBs) to review interim analyses.

* **E-Consent and Comprehension Verification (`SEC. 302(b)`)**
    * **Status:** Design Required.
    * **Description:** A module to manage the electronic informed consent process.
    * **Technical Considerations:** Must provide a version-controlled, auditable e-consent workflow. Crucially, as mandated by the Act, it **must include an interactive, configurable comprehension quiz** that patients must pass before being able to sign the consent form. The record of successful quiz completion and the signed consent form must be immutably logged.

* **Clinical Site Integration (eSource/EDC Module)**
    * **Status:** Design Required.
    * **Description:** A module to facilitate streamlined data capture directly from clinical sites, minimizing the burden on hospital staff.
    * **Technical Considerations:**
        * **EHR Integration:** Must provide robust tools for bi-directional EHR integration (e.g., using HL7 FHIR APIs) to automate the capture of pre-defined clinical outcomes from existing records (eSource).
        * **Electronic Case Report Forms (eCRF):** Must offer a simple, web-based eCRF interface for clinicians to manually enter the minimal data required by pragmatic protocols, for sites where direct EHR integration is not feasible.
        * **Workflow Support:** Should integrate with clinical workflows, for example, by providing notifications within the EHR for eligible patients or automating lab test orders required by a protocol.

* **Investigational Product (IP) Logistics Integration (`SEC. 204(d)(4)`)**
    * **Status:** Design Required.
    * **Description:** A module to coordinate the dispatch and delivery of investigational products to patients or clinical sites.
    * **Technical Considerations:** Requires secure APIs to integrate with third-party pharmacy, specialty courier, and laboratory systems. Must be able to trigger shipments and track delivery status, linking this information to the Blockchain Supply-Chain Ledger.

### 3.4 Data Ownership & Access Control

User-centric control and robust security are paramount, designed to be managed by users and auditable by the community and governing bodies.

* **Identity and Authentication:**
    * **Authentication Service:** **AWS Cognito** or self-hosted **Keycloak/Hydra** providing OAuth 2.0/OIDC services. Enforces MFA.
    * **Decentralized Identity:** The platform will support a self-sovereign identity model based on W3C standards for **Decentralized Identifiers (DIDs)** and **Verifiable Credentials (VCs)**. This allows patients to control their identity and share specific, verified claims (e.g., proof of diagnosis) without revealing underlying data. Candidate frameworks for integration include **Hyperledger Aries/Indy** and **SpruceID's DIDKit**.
* **Authorization:** Combination of **RBAC** (defined roles: `data_owner`, `data_custodian` (e.g., parent/guardian), `researcher`, `clinician_delegate`, `app_developer`, `admin`) and **ABAC** enforced via a policy engine (e.g., Open Policy Agent integrated at the API Gateway/backend). Policies use attributes like user roles, consent scope, data sensitivity level, and requested resource category to make dynamic authorization decisions.
* **Permission/Consent Management:** A core module providing user-facing interfaces (via the reference portal and APIs for third-party apps) for viewing data access logs, managing connected applications, and granting/revoking fine-grained consents. Consents are persisted in a dedicated database table detailing grantee, scope, duration, and status, with hashes of consent transactions logged to the immutable ledger.
* **Third-Party App Credentials:** Secure, encrypted storage (e.g., **AWS Secrets Manager** or **HashiCorp Vault**) for user-provided OAuth tokens/API keys needed by connector plugins to access external services (like Fitbit, Oura). Access is tightly controlled via IAM roles granted only to the specific plugin execution environments authorized by the user.
* **Auditing:** An immutable audit log (e.g., AWS QLDB or an append-only table with cryptographic verification) shall record all authentication events, authorization decisions, consent changes, data access requests (including denied attempts), and administrative actions. This log provides the transparency necessary for oversight by the TSC and the global community.

### 3.5 Data Storage

Layered approach prioritizing raw data integrity and optimized query performance.

* **Raw Data Storage (AWS S3):**
    * Bucket Structure: `s3://dfda-raw-data-{env}/{user_id_hash_prefix}/{user_id}/{source_type}/{yyyy}/{mm}/{dd}/{file_uuid}.{ext}` (Hashing prefix avoids S3 key hotspots).
    * Encryption: **SSE-KMS** mandatory. Consider client-side encryption support via SDK for highly sensitive data sources.
    * Access Control: Bucket policies & IAM restrict access strictly to authorized service roles (e.g., Ingestion API role, Mapping Engine role). Direct user access disallowed. Versioning enabled.
    * Lifecycle Policies: Define policies for transitioning older raw data to lower-cost storage tiers (e.g., S3 Infrequent Access, Glacier) or eventual deletion based on data retention policies.
    * Backup: Leverage managed service provider's automated backups, point-in-time recovery (PITR), and potentially cross-region snapshots for disaster recovery.
* **Time-Series Data Storage (TimescaleDB on Managed PostgreSQL):**
    * Schema (`observations` Hypertable - Key Fields): See previous elaboration. Add `metadata` JSONB field for source-specific context.
    * Partitioning: Automatic time partitioning (e.g., weekly/monthly chunks) via TimescaleDB. Consider composite partitioning including `user_id` or `variable_id` based on query analysis during performance tuning.
    * Indexes: Primary composite `(user_id, variable_id, timestamp DESC)`. Indexes on `timestamp`, `variable_id`, potentially GIN index on `metadata`.
    * Data Retention: Configurable via platform settings, potentially tiered (e.g., retain X years in primary hypertable, archive older to S3/cheaper storage, delete after Y years according to consent/policy). Needs robust implementation.
    * Backup: Leverage managed service provider's automated backups, point-in-time recovery (PITR), and potentially cross-region snapshots for disaster recovery.
* **Metadata / Relational Storage (PostgreSQL):**
    * Schema: Maintain detailed ERD and DDL scripts in version control. Enforce referential integrity. Use UUIDs widely. Store hashes of consent documents, not the documents themselves.

### 3.6 Data Mapping & Validation Engine

Handles heterogeneity of input data asynchronously.

* **Implementation:** Pool of containerized **Python microservices** on EKS, scaling based on SQS queue depth.
* **Mapping Logic:**
    * **Core Standards:** The platform will standardize on the **HL7 FHIR (R4/R5)** standard for clinical data and the **OMOP Common Data Model** for observational research.
    * **Core Mappers:** The reference implementation will leverage established open-source tools like **HAPI FHIR** (for FHIR data processing) and the **OHDSI tool stack** (for OMOP-based analytics). Prizes will be offered to extend these with robust mappers for **HL7v2, C-CDA, and major wearable APIs (Fitbit, Oura, Garmin)**.
    * **Plugin Interface:** Defined API for plugins to register custom mappers for specific file types or third-party APIs (see Section 4).
    * **Reference Data Usage:** Crucial reliance on `variables` and `units` tables for mapping source terms/units to standardized internal IDs. Handles code system translations (e.g., local lab code to LOINC).
* **Validation Logic:** Performed *after* mapping to standardized format. Checks against `variables` definitions (range, units). Implements outlier detection algorithms (configurable, e.g., IQR, Z-score) flagging `is_outlier`. Configurable severity levels for validation failures (log, flag, reject).
* **Error Handling:** Detailed structured logging of errors (input data ref, mapper/validator step, error type). Use of SQS dead-letter queues for persistent failures requiring manual investigation. API endpoint for users/systems to query job status and error details.
* **Asynchronous Queue (SQS):** Use standard SQS queues. Consider FIFO queues if strict processing order is essential for certain data types, but acknowledge potential throughput limitations.
* **API Specs (OAS v3.x):**
    * *Core -> Plugin:* Defined interfaces for triggering specific plugin types (e.g., `POST /plugin/v1/map`, `POST /plugin/v1/analyze`, `GET /plugin/v1/visualize`). Core provides necessary context (data query parameters, user context).
    * *Plugin -> Core:* Plugins use Core APIs (Ingestion, Data Retrieval, User Management) via standard OAuth tokens. Data Retrieval API (separate spec needed) allows plugins to query user data based on granted consent scope.
* **Data Formats:** JSON for APIs. Analysis/Visualization plugins receive data in a standardized tabular format (e.g., Pandas DataFrame structure delivered via API response or temporary file access).
* **Security:** Plugins execute in isolated, containerized environments (e.g., Lambda, specific EKS pods) with least-privilege IAM roles. Core platform rigorously validates plugin requests against user consent *before* executing or providing data access. Network policies restrict plugin communication.
* **Registration/Discovery:** Plugin Registry stores metadata, including required OAuth scopes, input/output data schemas, and execution endpoints. Core uses this to validate and route requests.

## 4. Plugin Framework Interfaces

Enables modular extension of platform capabilities.

* **API Specs (OAS v3.x):**
    * *Core -> Plugin:* Defined interfaces for triggering specific plugin types (e.g., `POST /plugin/v1/map`, `POST /plugin/v1/analyze`, `GET /plugin/v1/visualize`). Core provides necessary context (data query parameters, user context).
    * *Plugin -> Core:* Plugins use Core APIs (Ingestion, Data Retrieval, User Management) via standard OAuth tokens. Data Retrieval API (separate spec needed) allows plugins to query user data based on granted consent scope.
* **Data Formats:** JSON for APIs. Analysis/Visualization plugins receive data in a standardized tabular format (e.g., Pandas DataFrame structure delivered via API response or temporary file access).
* **Security:** Plugins execute in isolated, containerized environments (e.g., Lambda, specific EKS pods) with least-privilege IAM roles. Core platform rigorously validates plugin requests against user consent *before* executing or providing data access. Network policies restrict plugin communication.
* **Registration/Discovery:** Plugin Registry stores metadata, including required OAuth scopes, input/output data schemas, and execution endpoints. Core uses this to validate and route requests.

## 5. Public-Facing and Oversight Applications

This section specifies the key applications that will be built on top of the core backend infrastructure to serve end-users, researchers, and regulators.

* **Public Knowledge Base ("Clinipedia") (`SEC. 204(d)(6)`)**
    * **Status:** Design Required.
    * **Description:** A public-facing web application that serves as the primary interface to the platform's synthesized findings. It will present the ranked lists of treatments and standardized "Outcome Labels" for any given condition.
    * **Technical Considerations:**
        * Requires a robust data-querying backend, a sophisticated but intuitive user interface for exploring complex health data, and high-availability architecture.
        * As mandated by `SEC. 204(d)(5)`, the **source code, feature weights, and a reproducible computational notebook for each version of the QALY-ranking algorithm** must be published and linked from this portal.
        * The implementation should draw inspiration from projects like the **Psychonaut Wiki**, potentially using a modified **MediaWiki** backend capable of storing and querying quantitative metadata alongside prose.
        * The system shall include functionality to **automatically generate and publish meta-analyses** based on the aggregated results of trials conducted on the platform and data imported from external registries like `ClinicalTrials.gov`.
        * Autonomous **AI agents shall be utilized to continuously scan, aggregate, and summarize new research** from public sources to help populate and update the knowledge base.
        * A **content governance model** (e.g., a reputation-weighted voting system) must be developed to ensure the integrity and trustworthiness of the user- and AI-generated information, separate from the code governance of the TSC.

* **Live Analytics Dashboards (`SEC. 204(c)(5)`)**
    * **Status:** Design Required.
    * **Description:** A suite of secure, role-based dashboards for trial sponsors, regulators, and IRBs to monitor trial enrollment, compliance, and blinded efficacy metrics in real-time.
    * **Technical Considerations:** Requires a real-time data pipeline from the core database, configurable visualizations, and strict access controls to ensure data segregation and appropriate blinding.

* **Ethical Oversight (IRB/DERB) Portal (`SEC. 302(a)`)**
    * **Status:** Design Required.
    * **Description:** A secure portal for members of Institutional Review Boards (IRBs) and Decentralized Ethical Review Boards (DERBs) to review trial protocols, monitor ongoing trials, and manage their oversight responsibilities.
    * **Technical Considerations:** Requires document management features, secure communication channels, and role-based access to specific trial data relevant to their oversight function.

* **Interoperability with National Systems (`SEC. 204(m)`)**
    * **Status:** Design Required.
    * **Description:** A dedicated layer to ensure the platform can securely and efficiently exchange data with other key national health initiatives.
    * **Technical Considerations:** Requires the development of specific API connectors and data mapping modules for systems including the **FDA's Sentinel Initiative**, the **NIH's *All of Us* Research Program**, and **CDC surveillance systems**. Must implement privacy-preserving linkage techniques and adhere to the specific data standards of each partner system.

* **Regulatory Transparency & Modeling Platform (`SEC. 406`)**
    * **Status:** Design Required.
    * **Description:** An application suite to host public justification reports for FDA regulatory actions and to develop the advanced modeling capabilities required to generate them.
    * **Technical Considerations:**
        * **Reporting Portal:** A public-facing application for publishing, searching, and commenting on regulatory action reports.
        * **Modeling Workbench:** A secure, AI-driven environment for conducting the **comprehensive, quantitative health and economic impact analyses** mandated by the Act. This system must be designed to be run by advanced autonomous AI agents with human oversight and have its models and outputs be fully auditable and open-source.

## 6. Governance and Automation Systems

This section specifies the systems required to fulfill the unique governance and automation mandates of the Act.

* **AI-Augmented Governance (`SEC. 204(g)`)**
    * **Status:** Design Required.
    * **Description:** An AI-driven system to analyze pull requests to the platform's public source code repository, assign risk scores, and automate the merge/rejection process based on TSC-defined rules.
    * **Technical Considerations:** Requires integration with repository webhooks (e.g., GitHub Actions), development or integration of a code-analysis AI model, and a rules engine to manage the triage and voting logic for the TSC.

* **Public Bounty Program (`SEC. 204(i)`)**
    * **Status:** Design Required.
    * **Description:** A system to manage the public bounty program for code contributions and vulnerability disclosures.
    * **Technical Considerations:** The long-term goal for the bounty and governance programs is to migrate their operations to a **Decentralized Autonomous Organization (DAO)** structure to ensure transparent, community-driven control. Frameworks like **Aragon** are candidates for this implementation.

## 7. Security Implementation

Multi-layered security meeting regulatory requirements.

* **Network Security:** As previously specified (VPC, Security Groups, WAF, Private Subnets).
* **Application Security:** As previously specified (SAST, DAST, Dependency Scanning, Input Validation, Rate Limiting). Focus on OWASP Top 10.
* **Data Security:** As previously specified (SSE-KMS, TLS 1.2+, TDE option, Application-level encryption considered for specific PII). Strict IAM policies. Data masking/de-identification capabilities for analytical exports where appropriate.
* **Authentication & Authorization:** As previously specified (MFA, short-lived tokens, OIDC, RBAC+ABAC).
* **Compliance:** Explicit controls mapped to **HIPAA** Security Rule (Administrative, Physical, Technical Safeguards) and **GDPR** articles (Lawful Basis, Data Subject Rights, Security of Processing). Regular internal/external audits. BAAs with cloud providers.
* **Incident Response:** Defined plan, regular testing (tabletop exercises). [Link to Plan](../operations/security/incident-response-plan.md)

## 8. Infrastructure & Deployment

Automated, repeatable, and resilient infrastructure.

* **Infrastructure as Code (IaC):** **Terraform** mandatory for all cloud resources.
* **CI/CD Pipeline:** **GitLab CI/CD / GitHub Actions** with automated testing (unit, integration, end-to-end), security scans, container builds, and promotion through environments (`dev` -> `staging` -> `prod`). Manual approval gate for production deployments.
* **Environments:** Isolated AWS accounts or VPCs for `dev`, `staging`, `prod`. Use of realistic (anonymized/synthetic) data in staging for testing.
* **Scalability Strategy:** As previously specified (Stateless services, HPA, RDS Read Replicas, potential TimescaleDB clustering, ALB).
* **Disaster Recovery:** As previously specified (Automated backups, cross-region replication strategy, IaC for redeployment, documented DR plan with RPO/RTO targets). Regular DR testing.
* **Data Retrieval API:** Support 500 concurrent complex analytical queries.
* **Scalability Targets:** Design to handle 5x current projected peak load; demonstrate horizontal scalability.

## 9. Appendices

*(Placeholders - Content to be generated separately or by human teams)*

* **Glossary of Technical Terms:** [Link](../reference/glossary.md)
* **Detailed Diagrams:** [Link](../architecture/diagrams/README.md)
* **Data Dictionary / Database Schema:** [Link](../reference/data-dictionary.md)
* **API Specifications (OAS Files):** [Link](../features/api/README.md)

---

*This specification is a living document and will be updated as technical decisions are made and implementation progresses.*
